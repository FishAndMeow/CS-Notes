# 数据挖掘导论 笔记 

**肖飞宇**   [feiyuxiaothu@gmail.com]()  

## chapter1 绪论

### 什么是数据挖掘

**数据挖掘和知识发现** 

![数据库中知识发现过程](https://i.loli.net/2018/12/05/5c077449b9a8d.png)

### 数据挖掘要解决的问题

+ 可伸缩：对于海量数据集进行处理并且需要实现新的数据结构从而对于每个记录进行有效访问，如果数据不能放入内存，需要开发非内存算法，使用诸如抽样技术或者开发并行和分布算法
+ 高维性
+ 异种数据和复杂数据
+ 数据的所有权和分布
+ 非传统的分析

![](https://i.loli.net/2018/12/05/5c07755664ce2.png)

### 数据挖掘任务

+ 预测任务：independent variable $\to$ dependent variable or explanatory variable $\to$ target variable
+ 预测建模：分类 和 回归
+ 关联分析：用来发现描述数据中强关联特征的模式，所发现的模式通常用蕴含规则或特征自己的形式表示
+ 聚类分析
+ 异常检测：目的在于发现真正的离群点(anomaly,outlier)，从而避免对于正常的对象标注为异常点

## Chapter 2 数据

### 数据类型

数据对象用一组刻画对象基本特征的**属性**描述

+ 属性和度量

  + 属性是对象的性质和特征
  + 而对于属性的明确定义和分析需要引入**测量标度**
  + 属性的不同类型：标称、序数、区间和比率

  ![](https://i.loli.net/2018/12/05/5c0777a6ab93e.png)

  + 用属性值来描述属性：离散、连续
  + 在关联分析中，特别需要注意只有非零值才重要的二元属性，称为非对称的二元属性

+ 数据集的类型

  + 一般特性
    + 维度：对于高维数据有时会发生 curse of dimensionality 所以在预处理的时候一个重要的步骤就是 数据降维
    + 稀疏性：稀疏性将大大节省计算量和存储空间（显然，只有非零的值才需要处理）
    + 分辨率：在不同的测量的精度下，数据会相当不同
  + 数据的记录：事务数据、数据矩阵、稀疏数据矩阵
  + 基于图形的数据
    + 带有数据之间联系的数据，如对于网页分析中互相链接的网页集
  + 有序数据
    + 时序数据
    + 序列数据
    + 时间序列数据：注意当两个数据测量的时间很接近时，需要考虑时间自相关性
    + 空间序列数据：空间自相关

### 数据质量

数据统计一定有误差和错误，所以数据挖掘需要进行

1. 数据质量问题的检查和改正
2. 使用可以容忍低质量数据的算法（对噪声的鲁棒性）

此处进行对于数据质量的考察，常常称为**数据清洗**

+ 测量和数据收集问题
  + 测量误差和数据收集错误
  + 噪声和伪像：噪声常常用于包含时间和空间分量的数据，此时，可以使用信号和图像处理技术降低噪声，尽管如此，消除噪声常常是困难的，所以诸多工作都希望设计**鲁棒算法** 另外，数据错误可能是更确定现象的结果，如一组照片在同一地方出现条纹，即为伪像
  + 精度、偏差和准确率
    + 精度(precision)：同一个量的重复测量值之间的接近程度
    + 偏差(bias)：测量值和被测量之间的系统的变差
    + 准确率(accuracy)：被测量量和实际值之间的接近度 其中一个重要的点是对于**有效数字**的使用
  + 离群点：对于**噪声**和**离群点**的区分十分重要，其反映了不同于数据集中其他大部分数据对象的特征的数据对象，或者相对于该属性的典型值来说不寻常的数值，所以有的时候离群点本身是关心的对象
  + 遗漏值
    + 删除遗漏值
    + 估计遗漏值（插值）
    + 分析时忽略遗漏值
  + 重复数据的**去重**

### 数据预处理

+ 聚集(aggregation)：将两个或者多个对象合并成为单个对象

+ 抽样：选择数据子集进行分析 但是**样本必须具有代表性**

  + 抽样方法：简单随机抽样、分层抽样

  抽样可能造成数据信息损失

  ![](https://i.loli.net/2018/12/05/5c077c9a362f5.png)

  抽样也需要**确定适当的样本容量**

  + 自适应渐进抽样

+ 降维

  + 维度灾难：随着维度增加，数据分析变得困难，数据在其的空间中越来越稀疏
  + 降维技术：**主成分分析**(PCA)用于连续属性找出原属性的线性组合并保证其相互正交得到新属性（主成分）**奇异值分解**(SVD)

+ 特征子集选择：采用仅使用特征的一个子集辣进行降维，对于存在**冗余特征**和**不相关特征**时很有效

  + 嵌入方法

  + 过滤方法

  + 包装方法

  + 特征子集的选择体系：特征选择过程是由子集评估度量、控制新的特征子集产生的搜索策略、停止搜索判断和验证组成 

    ![](https://i.loli.net/2018/12/05/5c077ea1f0ba8.png)

  + 特征加权：特征越重要其权重就越大

+ 特征创建：由原来的属性创建新的属性集，更有效地捕捉数据集中的重要信息

  + 特征提取：如在人脸识别中进行相关的边和区域的提取

  + 映射数据到新的空间：如傅里叶变换

    ![](https://i.loli.net/2018/12/05/5c077f3fb992a.png)

  + 特征构造

+ 离散化和二元化：在一些诸如分类和关联算法中需要数据是二元属性模式，从而需要将连续属性变换为分类属性

  + 二元化

  + 连续属性离散化：指定分割点 其根本的区别是**对于类信息的使用**

    + 非监督离散化：不使用类信息进行一些简单的分割，如等宽、等频率和等深，及k均值

      ![](https://i.loli.net/2018/12/05/5c078282b6f7d.png) 

    + 监督离散化：可以采用**极大化区间纯度**的方法进行分割点的确定

      首先定义 **熵**(enropy) 设 k 为不同的类标号数， $m_i$是某划分的第 i 个区间中值的个数，$m_{ij}$是区间 i 中类 j 的值的个数，下式给出了第 i 个区间的熵 $e_i$ 
      $$
      e _ { i } = \sum _ { i = 1 } ^ { k } p _ { i j } \log _ { 2 } p 
      $$
      其中， $p_{ij} = m_{ij}/m_i$ 是第 i 个区间中类 j 的比率，该划分的总熵是每个区间的熵的加权平均，即
      $$
      e = \sum _ { i = 1 } ^ { n } w
      $$
      直观上，**熵是区间纯度的度量**

    + 具有过多值的分类属性

  + 变量变换

    + 简单函数变换
    + 规范化和标准化

### 相似性和相异性的度量

采用**邻近度**进行度量

+ 基础

  + 定义
  + 变换：通常可以进行一些区间变换

+ 数据对象之间的相异度

  + 距离的定义
    $$
    d ( \mathbf { x } , \mathbf { y } ) = \sqrt { \sum _ { k = 1 } ^ { n } \left( x _ { k } - y _ { k } \right) ^ { 2 } }
    $$
    同样可以推广为明可夫斯基距离
    $$
    d ( \mathbf { x } , \mathbf { y } ) = \left( \sum _ { k = 1 } ^ { n } \left| x _ { k } - y _ { k } \right| ^ { r } \right) ^ { 1 / r }
    $$

    + r = 1 L1 范数

    + r =2 欧几里得距离 L2 范数

    + r = $\infty$ 上确界距离 $L_{\infty}$ 距离
      $$
      d ( \mathbf { x } , \mathbf { y } ) = \lim _ { r \rightarrow \infty } \left( \sum _ { k = 1 } ^ { n } \left| x _ { k } - y _ { k } \right| ^ { r } \right) ^ { 1 / r }
      $$

  + 距离的特征

    + 非负
    + 对称
    + 三角不等式

+ 数据对象之间的相似度：不成立三角不等式

+ 邻近度度量

  + 二元数据的相似度量：相似系数
    $$
    \begin{array} { l } { f _ { 00 } = \text { the number of attributes where } x \text { is } 0 \text { and } y \text { is } 0 } \\ { f _ { 01 } = \text { the number of attributes where } x \text { is } 0 \text { and } y \text { is } 1 } \\ { f _ { 10 } = \text { the number of attributes where } x \text { is } 1 \text { and } y \text { is } 0 } \\ { f _ { 11 } = \text { the number of attributes where } x \text { is } 1 \text { and } y \text { is } 1 } \end{array}
    $$

    + 简单匹配系数(SMC)
      $$
      S M C = \frac { \text { number of matching attribute values } } { \text { number of attributes } } = \frac { f _ { 11 } + f _ { 00 } } { f _ { 01 } + f _ { 10 } + f _ { 11 } + f _ { 00 } }
      $$

    + Jaccard系数：常常用于处理仅包含有非对称的二元属性的对象
      $$
      J = \frac { \text { number of matching presences } } { \text { number of attributes not involved in } 00 \text { matches } } = \frac { f _ { 11 } } { f _ { 01 } + f _ { 10 } + f _ { 11 } }
      $$

  + 余弦相似度：对于**文档相似度**进行度量的最常用的量之一
    $$
    \cos ( \mathbf { x } , \mathbf { y } ) = \frac { \mathbf { x } \cdot \mathbf { y } } { \| \mathbf { x } \| \| \mathbf { y } \| }
    $$

  + 广义Jaccard系数

  + 相关性

    两个具有二元变量或者连续变量的数据对象之间的相关性是对象属性之间线性联系的度量，两个数据对象 x 和 y 之间的 皮尔森相关系数可以定义为：
    $$
    \operatorname { corr } ( \mathbf { x } , \mathbf { y } ) = \frac { \text { covariance } ( \mathbf { x } , \mathbf { y } ) } { \text { standard. deviation } ( \mathbf { x } ) * \text { standard.deviation } ( \mathbf { y } ) } = \frac { s _ { x y } } { s _ { x } s _ { y } }
    $$
    其中
    $$
    ( \mathbf { x } , \mathbf { y } ) = s _ { x y } = \frac { 1 } { n - 1 } \sum _ { k = 1 } ^ { n } \left( x _ { k } - \overline { x } \right) \left( y _ { k } - \overline { y } \right)
    $$

    $$
    \begin{aligned} \text { standard.deviation } ( \mathbf { x } ) & = s _ { x } = \sqrt { \frac { 1 } { n - 1 } \sum _ { k = 1 } ^ { n } \left( x _ { k } - \overline { x } \right) ^ { 2 } } \\ \text { standard.deviation ( } \mathbf { y } ) & = s _ { y } = \sqrt { \frac { 1 } { n - 1 } \sum _ { k = 1 } ^ { n } \left( y _ { k } - \overline { y } \right) ^ { 2 } } \end{aligned}
    $$

    $$
    \begin{aligned} \overline { x } & = \frac { 1 } { n } \sum _ { k = 1 } ^ { n } x _ { k } \text { is the mean of } \mathbf { x } \\ \overline { y } & = \frac { 1 } { n } \sum _ { k = 1 } ^ { n } y _ { k } \text { is the mean of } \mathbf { y } \end{aligned}
    $$

    相关度总是在 1 和 -1 之间取值，相关度为 1 或者 -1 意味着 x 和 y 具有完全正（负）线性关系

    如果相关度为 0 表明不存在线性关系，但是可能存在非线性关系

  + Bregman 散度：损失或者失真函数

    可以假设，x 和 y 是两个点，其中 y 是原始的点， x 是某个失真或者近似，如 x 可能是添加了一些随机噪声到 y 上而产生，而损失函数的目的是测量用 x 近似 y 所产生的失真或者损失，显然，两者越类似，失真就越小，从而可以采用 Bregman 散度用于相异性函数

    **定义**

    给定一个严格凸函数 $\phi$ 由该函数生成的 Bregman散度（损失函数）$D(x,y)$为
    $$
    D ( \mathbf { x } , \mathbf { y } ) = \phi ( \mathbf { x } ) - \phi ( \mathbf { y } ) - \langle \nabla \phi ( \mathbf { y } ) , ( \mathbf { x } - \mathbf { y } ) \rangle
    $$
    $D(x,y)$ 可以写成 $D(x,y) = \phi(x) - L(x) , \quad L(x) = \phi(y) + \langle \nabla \phi ( \mathbf { y } ) , ( \mathbf { x } - \mathbf { y } ) \rangle$ 其中的 $L(x)$ 就代表在 y 上正切与函数 $\phi$ 的平面方程，即 $L(x)$为函数$\phi$在 y 附近的线性部分，Bregman散度就是一个函数和函数的线性近似之间差的余项

    *Example*

    若 $\phi(t) = t^2$ ，则 $D ( x , y ) = x ^ { 2 } - y ^ { 2 } - 2 y ( x - y ) = ( x - y ) ^ { 2 }$

    有

    ![](https://i.loli.net/2018/12/05/5c0789f8f2a37.png)

  + 邻近度计算

    + 距离度量的标准化和相关性

      需要处理当属性具有不同的值域的情形（即变量具有不同的尺度）：进行标准化处理

      另外当有的属性相关时，若其具有不同的值域（方差），并且数据分布近似于高斯（正态）分布，有 Mahalanobis 距离
      $$
      ( \mathbf { x } , \mathbf { y } ) = ( \mathbf { x } - \mathbf { y } ) \mathbf { \Sigma } ^ { - 1 } ( \mathbf { x } - \mathbf { y } ) ^ { T }
      $$
      其中 $\Sigma^{-1}$ 是数据协方差矩阵的逆

      ![](https://i.loli.net/2018/12/05/5c078b33e5c0f.png)

      *Fig* Set of two-dimensional points. The Mahalanobis distance between the two points represented by large dots is 6; their Euclidean distance is 14.7 

    + 组合异种属性的相似度的算法

      ![](https://i.loli.net/2018/12/05/5c078b9913550.png)

    + 使用权值：当某些属性对邻近度的定义更重要时，需要对于贡献加权
      $$
      ( \mathbf { x } , \mathbf { y } ) = \frac { \sum _ { k = 1 } ^ { n } w _ { k } \delta _ { k } s _ { k } ( \mathbf { x } , \mathbf { y } ) } { \sum _ { k = 1 } ^ { n } \delta _ { k } }
      $$
      同时，明可夫斯基距离定义变为
      $$
      d ( \mathbf { x } , \mathbf { y } ) = \left( \sum _ { k = 1 } ^ { n } w _ { k } \left| x _ { k } - y _ { k } \right| ^ { r } \right) ^ { 1 / r }
      $$






+ Insights
  + 邻近度度量的类型应该和数据类型适应。稠密、连续的数据通常使用距离度，如欧几里得距离。连续属性的邻近度通常用属性值的差表示，并且距离度量提供了一种将其组合到总的邻近度度量的良好方法
  + 对于稀疏数据，常常存在非对称的属性，通常使用忽略0-0匹配的相似性度量。这反应了这样一个基本事实：对于一对复杂对象，相似度依赖于他们共同具有的性质数目，而不是依赖于他们都缺失的性质数目
  + 在某些情况下，想要得到合适的相似度度量，数据的变换和规范化是重要的

## Chapter 3 探索数据

对数据的初步研究和分析，有助于选择合适的数据预处理和数据分析技术

### 汇总统计

采用量化的单个数或者数的小几个进行很大的集合的特征的捕获

+ 频率和众数

+ 百分位数

+ 位置度量：均值和中位数

  对连续数据，采用最为广泛的是**均值**和**中位数**，是值集位置的考量，另外还有截断均值的概念

+ 散布度量：极差和方差

  相比而言，方差更加可取，而标准差也很常用
  $$
  ( x ) = s _ { x } ^ { 2 } = \frac { 1 } { m - 1 } \sum _ { i = 1 } ^ { m } \left( x _ { i } - \overline { x } \right) ^ { 2 }
  $$
  同时可以定义绝对平均偏差、中位数绝对偏差和四分位数极差
  $$
  \begin{array} { c } { \operatorname { AAD } ( x ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left| x _ { i } - \overline { x } \right| } \\ { \operatorname { MAD } ( x ) = \operatorname { median } \left( \left\{ \left| x _ { 1 } - \overline { x } \right| , \ldots , \left| x _ { m } - \overline { x } \right| \right\} \right) } \\ { \text { interquartile range } ( x ) = x _ { 75 \% } - x _ { 25 \% } } \end{array}
  $$

+ 多元汇总统计

  对于多元数据的散布，更多的采用**协方差矩阵**，其中$S_{ij}$是第 i 个和第 j 个数据的协方差，有
  $$
  s _ { i j } = \operatorname { covariance } \left( x _ { i } , x _ { j } \right)
  $$
  其中
  $$
  \operatorname { covariance } \left( x _ { i } , x _ { j } \right) = \frac { 1 } { m - 1 } \sum _ { k = 1 } ^ { m } \left( x _ { k i } - \overline { x _ { i } } \right) \left( x _ { k j } - \overline { x _ { j } } \right)
  $$
  注意到， $\operatorname { covariance } \left( x _ { i } , x _ { i } \right) = \operatorname { variance } \left( x _ { i } \right)$ 

  两个属性的协方差是两个属性一起变化并依赖于变量大小的度量，其中协方差的值接近 0 代表两个变量不具有线性关系，但是不能仅仅依靠对于协方差的值的观测来确定两个变量之间的关联程度。相比之下，**相关性**更加可取，**相关矩阵**的元素$R_{ij}$代表数据的第 i 和第 j 个属性之间的相关性，有
  $$
  r _ { i j } = \text { correlation } \left( x _ { i } , x _ { j } \right) = \frac { \text { covariance } \left( x _ { i } , x _ { j } \right) } { s _ { i } s _ { j } }
  $$
  矩阵对角线元素均为1，而其余取值在 -1 和 1之间

### 可视化

+ 少量数据的可视化
  + 茎叶图
  + 直方图
  + 盒状图
  + 百分位图和经验累积分布函数
  + 散点图
+ 可视化时空数据
  + 等高线图
  + 曲面图
  + 矢量图
  + 低维切片
+ 高维数据的可视化

### 多维数据分析

## 分类：基本概念、决策树和模型评估

## 分类：其他技术

## 关联分析：基本概念和算法

## 关联分析：高级概念

## 聚类分析：基本概念和算法

## 聚类分析：其他问题和算法

## 异常检测

