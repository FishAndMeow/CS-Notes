{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "+ Inputs are often called the *predictors*, also *independent variables*. And in the pattern recognition the term *features* is preferred.\n",
    "\n",
    "+ Outputs are called the *response*, or the *dependent variables*.\n",
    "\n",
    "# Variable Types and Terminology\n",
    "\n",
    "Distinction in output ype has led to a naming convection for prediction tasks:\n",
    "\n",
    "+ *regression*  when we predict quantitative outputs\n",
    "\n",
    "+ *classification* when we predict qualitative outputs\n",
    "\n",
    "A third type is *ordered categorical*, such as small, medium and large, where there is an ordering between the values, but no metric notion is appropriate(the difference between medium and small need not be the\n",
    "same as that between large and medium).\n",
    "\n",
    "# Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors\n",
    "\n",
    "The linear model makes huge assumptions about structure and yields stable\n",
    "but possibly inaccurate predictions. The method of k-nearest neighbors\n",
    "makes very mild structural assumptions: its predictions are often accurate\n",
    "but can be unstable.\n",
    "\n",
    "## Linear Models and Least Squares\n",
    "\n",
    "Given a vector of inputs $X ^ { T } = \\left( X _ { 1 } , X _ { 2 } , \\ldots , X _ { p } \\right)$, we predict the output $Y$ via the model\n",
    "\n",
    "$$\n",
    "\\hat { Y } = \\hat { \\beta } _ { 0 } + \\sum _ { j = 1 } ^ { p } X _ { j } \\hat { \\beta } _ { j }\n",
    "$$\n",
    "\n",
    "The term $\\hat{\\beta}_0$ is the **intercept**, also known as the *bias* in machine learning.\n",
    "\n",
    "How do we fit the linear model to a set of training data? There are\n",
    "many different methods, but by far the most popular is the method of\n",
    "least squares. In this approach, we pick the coefficients $\\beta$ to minimize the\n",
    "residual sum of squares.\n",
    "\n",
    "$$\n",
    "\\operatorname { RSS } ( \\beta ) = ( \\mathbf { y } - \\mathbf { X } \\beta ) ^ { T } ( \\mathbf { y } - \\mathbf { X } \\beta )\n",
    "$$\n",
    "\n",
    "if $\\mathbf { X } ^ { T } \\mathbf { X }$ is nonsingular, then the unique solution is given by\n",
    "\n",
    "$$\n",
    "\\hat { \\beta } = \\left( \\mathbf { X } ^ { T } \\mathbf { X } \\right) ^ { - 1 } \\mathbf { X } ^ { T } \\mathbf { y }\n",
    "$$\n",
    "\n",
    "## Nearest-Neighbor Methods\n",
    "\n",
    "Nearest-neighbor methods use those observations in the training set $T$ closest in input space to x to form $\\hat{Y}$ . Specifically, the k-nearest neighbor fit\n",
    "for $\\hat{Y}$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat { Y } ( x ) = \\frac { 1 } { k } \\sum _ { x _ { i } \\in N _ { k } ( x ) } y _ { i }\n",
    "$$\n",
    "\n",
    "where $N_k(x)$ is the neighborhood of x defined by the k closest points $x_i$ in the training sample.Closeness implies a metric, which for the moment we assume is Euclidean distance. So, in words, we find the k observations with $x_i$ closest to x in input space, and average their response.\n",
    "\n",
    "## From Least Squares to Nearest Neighbors\n",
    "\n",
    "The linear decision boundary from least squares is very smooth, and apparently stable to fit. It does appear to rely heavily on the assumption that a linear decision boundary is appropriate. In language we will develop\n",
    "shortly, it has **low variance and potentially high bias**.\n",
    "\n",
    "On the other hand, the k-nearest-neighbor procedures do not appear to\n",
    "rely on any stringent assumptions about the underlying data, and can adapt\n",
    "to any situation. However, any particular subregion of the decision boundary depends on a handful of input points and their particular positions, and is thus wiggly and unstable—**high variance and low bias**.\n",
    "\n",
    "# Statistical Decision Theory\n",
    "\n",
    "Let $X \\in \\mathbb { R } ^ { p }$ denote a real valued random input vector, and $Y \\in \\mathbb { R } ^ { p }$ a real valued random output variable, with joint distribution $Pr(X,Y)$. This theory requires a **Loss Function** $L(Y,f(X0)$ for penalizing errors in prediction, and by far the most common and convenient is *squared error loss*: $L ( Y , f ( X ) ) = ( Y - f ( X ) ) ^ { 2 }$. This leads us to a criterion for chossing $f$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\operatorname { EPE } ( f ) & = \\mathrm { E } ( Y - f ( X ) ) ^ { 2 } \\\\ & = \\int [ y - f ( x ) ] ^ { 2 } \\operatorname { Pr } ( d x , d y ) \\end{aligned}\n",
    "$$\n",
    "the expected(squared) prediction error. By conditioning on X, we can write EPE as\n",
    "\n",
    "$$\n",
    "\\operatorname { EPE } ( f ) = \\mathrm { E } _ { X } \\mathrm { E } _ { Y | X } \\left( [ Y - f ( X ) ] ^ { 2 } | X \\right)\n",
    "$$\n",
    "\n",
    "and we see that it suffices to minimize EPE pointwise:\n",
    "\n",
    "$$\n",
    "f ( x ) = \\operatorname { argmin } _ { c } \\mathrm { E } _ { Y | X } \\left( [ Y - c ] ^ { 2 } | X = x \\right)\n",
    "$$\n",
    "\n",
    "The solution is \n",
    "\n",
    "$$\n",
    "f ( x ) = \\mathrm { E } ( Y | X = x )\n",
    "$$\n",
    "\n",
    "the conditional expectation, also known as the regression function. Thus\n",
    "the best prediction of Y at any point X = x is the conditional mean, when\n",
    "best is measured by average squared error.\n",
    "\n",
    "We conclude that both k-nearest neighbors and least squares end up approximating\n",
    "conditional expectations by averages. But they differ dramatically in terms\n",
    "of model assumptions:\n",
    "\n",
    "+ Least squares assume $f(x)$ is well approximated by a globally linear\n",
    "function.\n",
    "\n",
    "+ k-nearest neighbors assumes $f(x)$ is well approximated by a locally\n",
    "constant function.\n",
    "\n",
    "Many of the more modern techniques described in this book are model\n",
    "based, although far more flexible than the rigid linear model. For example,\n",
    "additive models assume that\n",
    "\n",
    "$$\n",
    "f ( X ) = \\sum _ { i = 1 } ^ { p } f _ { j } \\left( X _ { j } \\right)\n",
    "$$\n",
    "\n",
    "This retains the additivity of the linear model, but each coordinate function\n",
    "$f_j$ is arbitrary.It turns out that the optimal estimate for the additive model\n",
    "uses techniques such as k-nearest neighbors to approximate univariate conditional expectations simultaneously for each of the coordinate functions.\n",
    "Thus the problems of estimating a conditional expectation in high dimensions are swept away in this case by imposing some (often unrealistic) model\n",
    "assumptions, in this case additivity.\n",
    "\n",
    "If we replace the $L_2$ loss function with the $L_1$:$\\mathrm { E } | Y - f ( X ) |$, and the solution is the conditional median\n",
    "\n",
    "$$\n",
    "\\hat { f } ( x ) = \\operatorname { median } ( Y | X = x )\n",
    "$$\n",
    "\n",
    "which is a different measure of location, and its estimates are more robust\n",
    "than those for the conditional mean. $L_1$ criteria have **discontinuities in\n",
    "their derivatives**, which have hindered their widespread use.\n",
    "\n",
    "Next,What do we do when the output is a categorical variable G? The same\n",
    "paradigm works here, except we need a different loss function for penalizing\n",
    "prediction errors. An estimate $\\hat{G}$ will assume values in $\\mathrm{g}$, the set of possible classes. Our loss function can be represented by a $K \\times K$ matrix $\\mathrm{L}$, where $K = card(\\mathrm{g})$. $L$ will be zero on the diagonal and nonnegative elsewhere,where $L(k, l)$ is the price paid for classifying an observation belonging to class $g_k$ as $g_l$.Most often we use the zero–one loss function, where all\n",
    "misclassifications are charged a single unit. The expected prediction error\n",
    "is\n",
    "\n",
    "$$\n",
    "\\mathrm { EPE } = \\mathrm { E } [ L ( G , \\hat { G } ( X ) ) ]\n",
    "$$\n",
    "\n",
    "Again\n",
    "\n",
    "$$\n",
    "\\mathrm { EPE } = \\mathrm { E } _ { X } \\sum _ { k = 1 } ^ { K } L \\left[ \\mathcal { G } _ { k } , \\hat { G } ( X ) \\right] \\operatorname { Pr } \\left( \\mathcal { G } _ { k } | X \\right)\n",
    "$$\n",
    "\n",
    "and again it suffices to minimize EPE pointwise:\n",
    "\n",
    "$$\n",
    "\\hat { G } ( x ) = \\operatorname { argmin } _ { g \\in \\mathcal { G } } \\sum _ { k = 1 } ^ { K } L \\left( \\mathcal { G } _ { k } , g \\right) \\operatorname { Pr } \\left( \\mathcal { G } _ { k } | X = x \\right)\n",
    "$$\n",
    "\n",
    "with the 0-1 loss function this simplifies to\n",
    "\n",
    "$$\n",
    "\\hat { G } ( x ) = \\operatorname { argmin } _ { g \\in \\mathcal { G } } [ 1 - \\operatorname { Pr } ( g | X = x ) ]\n",
    "$$\n",
    "\n",
    "or simply\n",
    "\n",
    "$$\n",
    "\\hat { G } ( x ) = \\mathcal { G } _ { k } \\text { if } \\operatorname { Pr } \\left( \\mathcal { G } _ { k } | X = x \\right) = \\max _ { g \\in \\mathcal { G } } \\operatorname { Pr } ( g | X = x )\n",
    "$$\n",
    "\n",
    "This reasonable solution is known as the *Bayes classifier*, and says that\n",
    "we classify to the most probable class, using the conditional (discrete) distribution $Pr(G|X)$.\n",
    "\n",
    "Again we see that the k-nearest neighbor classifier directly approximates\n",
    "this solution—a majority vote in a nearest neighborhood amounts to exactly this, except that conditional probability at a point is relaxed to conditional probability within a neighborhood of a point, and probabilities areestimated by training-sample proportions.\n",
    "\n",
    "# Local Methods in High Dimensions\n",
    "\n",
    "It would seem that with a reasonably large set of training data, we could always approximate the theoretically optimal conditional expectation by k-nearest-neighbor averaging, since we should\n",
    "be able to find a fairly large neighborhood of observations close to any x\n",
    "and average them. This approach and our intuition breaks down in high\n",
    "dimensions, and the phenomenon is commonly referred to as the **curse\n",
    "of dimensionality**.\n",
    "\n",
    "# Statistical Models, Supervised Learning and Function Approximation\n",
    "\n",
    "Our goal is to find a useful approximation $\\hat{f(x)}$ to the function $f(x)$ , but the previous models can have two disadvantages:\n",
    "\n",
    "+ if the dimension of the input space is high, the nearest neighbors need\n",
    "not be close to the target point, and can result in large errors;\n",
    "\n",
    "+ if special structure is known to exist, this can be used to reduce both\n",
    "the bias and the variance of the estimates.\n",
    "\n",
    "## A Statistical Model for the Joint Distribution Pr(X, Y )\n",
    "\n",
    "Suppose in fact that our data arose from a statistical model\n",
    "\n",
    "$$\n",
    "Y = f ( X ) + \\varepsilon\n",
    "$$\n",
    "\n",
    "where the random error $\\varepsilon$ has $E(\\varepsilon) = 0$ and is independent of X. The additive error model is a useful approximation to the truth. For\n",
    "most systems the input–output pairs (X, Y ) will not have a deterministic\n",
    "relationship Y = f(X). Generally there will be other unmeasured variables\n",
    "that also contribute to Y , including measurement error. The additive model\n",
    "assumes that we can capture all these departures from a deterministic relationship via the error $\\varepsilon$.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised learning attempts to learn f by\n",
    "example through a teacher. One observes the system under study, both\n",
    "the inputs and outputs, and assembles a training set of observations $T =\n",
    "(x_i, y_i), i = 1, . . . , N.$ The observed input values to the system xi are also\n",
    "fed into an artificial system, known as a learning algorithm (usually a computer program), which also produces outputs $\\hat{f(x_i)}$ in response to the inputs. The learning algorithm has the property that it can modify its input/output relationship $\\hat{f}$ in response to differences $y_i − \\hat{f(x_i)}$ between the\n",
    "original and generated outputs. This process is known as learning by example. Upon completion of the learning process the hope is that the artificial\n",
    "and real outputs will be close enough to be useful for all sets of inputs likely\n",
    "to be encountered in practice.\n",
    "\n",
    "## Function Approximation\n",
    "\n",
    "The approach taken in applied mathematics and statistics has been from the perspective of function approximation and estimation. Here the data pairs ${x_i, y_i}$ are viewed as points in a $(p + 1)$-dimensional Euclidean space. The function f(x) has domain equal to the p-dimensional input subspace, and is related to the data via a model.\n",
    "\n",
    "The goal is to obtain a useful approximation to f(x) for all x in some region of $\\mathbb { R } ^ { p }$, given the representations in $T$ .Although somewhat less glamorous than the learning paradigm, treating\n",
    "supervised learning as a problem in function approximation encourages the\n",
    "geometrical concepts of Euclidean spaces and mathematical concepts of\n",
    "probabilistic inference to be applied to the problem. \n",
    "\n",
    "Many of the approximations we will encounter have associated a set of\n",
    "parameters $\\theta$ that can be modified to suit the data at hand.  For example, the linear model $f ( x ) = x ^ { T } \\beta$ has $\\theta = \\beta$. Another class od useful approximators can be expressed as *linear basis expansions*\n",
    "\n",
    "$$\n",
    "f _ { \\theta } ( x ) = \\sum _ { k = 1 } ^ { K } h _ { k } ( x ) \\theta _ { k }\n",
    "$$\n",
    "\n",
    "where the $h_k$ are a suitable set of functions or transformations of the input\n",
    "vector x. Traditional examples are polynomial and trigonometric expansions, where for example $h_k$ can be $x_1^2$, $x_1x_2^2$,$\\cos(x_1)$ and so on. We\n",
    "also encounter nonlinear expansions, such as the sigmoid transformation\n",
    "common to neural network models\n",
    "\n",
    "$$\n",
    "h _ { k } ( x ) = \\frac { 1 } { 1 + \\exp \\left( - x ^ { T } \\beta _ { k } \\right) }\n",
    "$$\n",
    "\n",
    "We can use least squares to estimate the parameters $\\theta$ in $f(\\theta)$ as we did\n",
    "for the linear model, by minimizing the residual sum-of-squares\n",
    "\n",
    "$$\n",
    "\\operatorname { RSS } ( \\theta ) = \\sum _ { i = 1 } ^ { N } \\left( y _ { i } - f _ { \\theta } \\left( x _ { i } \\right) \\right) ^ { 2 }\n",
    "$$\n",
    "\n",
    "For the linear model we get a simple closed form solution to the minimization problem. This is also true for the basis function methods, if the\n",
    "basis functions themselves do not have any hidden parameters. Otherwise\n",
    "the solution requires either iterative methods or numerical optimization.\n",
    "\n",
    "While least squares is generally very convenient, it is not the only criterion used and in some cases would not make much sense. A more general principle for estimation is **maximum likelihood estimation**.\n",
    "\n",
    "Suppose we have a random sample $y _ { i } , i = 1 , \\dots , N$ from a density $\\operatorname { Pr } _ { \\theta } ( y )$ indexed by some parameters $\\theta$. The log-probability of the observed sample is\n",
    "\n",
    "$$\n",
    "L ( \\theta ) = \\sum _ { i = 1 } ^ { N } \\log \\operatorname { Pr } _ { \\theta } \\left( y _ { i } \\right)\n",
    "$$\n",
    "\n",
    "**The principle of maximum likelihood assumes that the most reasonable\n",
    "values for** $\\theta$ **are those for which the probability of the observed sample is\n",
    "largest.**\n",
    "\n",
    "Least squares for the additive error model $Y = f _ { \\theta } ( X ) + \\varepsilon$, with $\\varepsilon \\sim N \\left( 0 , \\sigma ^ { 2 } \\right)$, is equivalent to maximum likelihood using the conditional\n",
    "likelihood\n",
    "\n",
    "$$\n",
    "\\operatorname { Pr } ( Y | X , \\theta ) = N \\left( f _ { \\theta } ( X ) , \\sigma ^ { 2 } \\right)\n",
    "$$\n",
    "\n",
    "So although the additional assumption of normality seems more restrictive,\n",
    "the results are the same. The log-likelihood of the data is\n",
    "\n",
    "$$\n",
    "L ( \\theta ) = - \\frac { N } { 2 } \\log ( 2 \\pi ) - N \\log \\sigma - \\frac { 1 } { 2 \\sigma ^ { 2 } } \\sum _ { i = 1 } ^ { N } \\left( y _ { i } - f _ { \\theta } \\left( x _ { i } \\right) \\right) ^ { 2 }\n",
    "$$\n",
    "\n",
    "A more interesting example is the multinomial likelihood for the regression function $\\operatorname { Pr } ( G | X )$ for a qualitative output G. Suppose we have a model $\\operatorname { Pr } \\left( G = \\mathcal { G } _ { k } | X = x \\right) = p _ { k , \\theta } ( x ) , k = 1 , \\ldots , K$ for the conditional probability of each class given X, indexed by the parameter vector $\\theta$. Then the log-likelihood (also referred to as the **cross-entropy**) is\n",
    "\n",
    "$$\n",
    "L ( \\theta ) = \\sum _ { i = 1 } ^ { N } \\log p _ { g _ { i } , \\theta } \\left( x _ { i } \\right)\n",
    "$$\n",
    "\n",
    "and when maximized it delivers values of $\\theta$ that best conform with the data in this likelihood sense.\n",
    "\n",
    "# Structured Regression Models\n",
    "\n",
    "## Difficulty of the Problem\n",
    "\n",
    "Any method that attempts to produce locally varying functions in small isotropic neighborhoods will run\n",
    "into problems in high dimensions—again the curse of dimensionality. And\n",
    "conversely, all methods that overcome the dimensionality problems have an\n",
    "associated—and often implicit or adaptive—metric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions.\n",
    "\n",
    "# Classes of Restricted Estimators\n",
    "\n",
    "The variety of nonparametric regression techniques or learning methods fall\n",
    "into a number of different classes depending on the nature of the restrictions\n",
    "imposed. These classes are not distinct, and indeed some methods fall in\n",
    "several classes.\n",
    "\n",
    "Each of the classes has associated with it one\n",
    "or more parameters, sometimes appropriately called smoothing parameters,\n",
    "that control the effective size of the local neighborhood. Here we describe\n",
    "three broad classes.\n",
    "\n",
    "## Roughness Penalty and Bayesian Methods\n",
    "\n",
    "To be added.\n",
    "\n",
    "## Kernel Methods and Local Regression\n",
    "\n",
    "To be added.\n",
    "\n",
    "## Basis Functions and Dictionary Methods\n",
    "\n",
    "To be added.\n",
    "\n",
    "# Model Selection and the Bias–Variance Tradeoff\n",
    "Suppose the data arise from a model $Y = f ( X ) + \\varepsilon$, with $\\mathrm { E } ( \\varepsilon ) = 0$ and \n",
    "$\\operatorname { Var } ( \\varepsilon ) = \\sigma ^ { 2 }$. For simplicity here we assume that the values of xi in the sample are fixed in advance (nonrandom). The expected prediction error\n",
    "at $x_0$, also known as test or generalization error, can be decomposed:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\operatorname { EPE } _ { k } \\left( x _ { 0 } \\right) & = \\mathrm { E } \\left[ \\left( Y - \\hat { f } _ { k } \\left( x _ { 0 } \\right) \\right) ^ { 2 } | X = x _ { 0 } \\right] \\\\ & = \\sigma ^ { 2 } + \\left[ \\operatorname { Bias } ^ { 2 } \\left( \\hat { f } _ { k } \\left( x _ { 0 } \\right) \\right) + \\operatorname { Var } _ { \\mathcal { T } } \\left( \\hat { f } _ { k } \\left( x _ { 0 } \\right) \\right) \\right] \\\\ & = \\sigma ^ { 2 } + \\left[ f \\left( x _ { 0 } \\right) - \\frac { 1 } { k } \\sum _ { \\ell = 1 } ^ { k } f \\left( x _ { ( \\ell ) } \\right) \\right] ^ { 2 } + \\frac { \\sigma ^ { 2 } } { k } \\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "There are three terms in this expression. The first term $\\sigma^2$ is the irreducible error—the variance of the new test target—and is beyond our control, even if we know the true $f(x0)$.\n",
    "\n",
    "The second and third terms are under our control, and make up the\n",
    "mean squared error of $\\hat{f_k(x_0)}$ in estimating $f(x_0)$, which is broken down into a bias component and a variance component.The bias term is the\n",
    "squared difference between the true mean $f(x_0)$ and the expected value of\n",
    "the estimate $\\left[ \\mathrm { E } _ { \\mathcal { T } } \\left( f _ { k } \\left( x _ { 0 } \\right) \\right) - f \\left( x _ { 0 } \\right) \\right] ^ { 2 }$,where the expectation averages the\n",
    "randomness in the training data. This term will most likely increase with\n",
    "k, if the true function is reasonably smooth. For small k the few closest\n",
    "neighbors will have values $f \\left( x _ { ( \\ell ) } \\right)$ close to $f(x_0)$. so their average should be close to $f(x_0)$. As k grows, the neighbors are further away, and then\n",
    "anything can happen.\n",
    "\n",
    "The variance term is simply the variance of an average here, and decreases as the inverse of k. So as k varies, there is a **bias–variance tradeoff**.\n",
    "\n",
    "More generally, as the model complexity of our procedure is increased, the\n",
    "variance tends to increase and the squared bias tends to decrease.\n",
    "\n",
    "Typically we would like to choose our model complexity to trade bias\n",
    "off with variance in such a way as to minimize the test error. An obvious\n",
    "estimate of test error is the *training error* $\\frac { 1 } { N } \\sum _ { i } \\left( y _ { i } - \\hat { y } _ { i } \\right) ^ { 2 }$. Unfortunately\n",
    "training error is not a good estimate of test error, as it does not properly\n",
    "account for model complexity.\n",
    "\n",
    "![](https://github.com/FishAndMeow/CS-Notes/blob/master/DataAnalysis/StatisticalLearning/ESL/fig/bias-variance-tradeoff.PNG)\n",
    "\n",
    "\n",
    "\n",
    "The figure shows the typical behavior of the test and training error, as\n",
    "model complexity is varied. The training error tends to decrease whenever\n",
    "we increase the model complexity, that is, whenever we fit the data harder.\n",
    "However with too much fitting, the model adapts itself too closely to the\n",
    "training data, and will not generalize well (i.e., have large test error). In\n",
    "that case the predictions $\\hat{f(x_0)}$ will have large variance, as reflected in the\n",
    "last term of expression equation. In contrast, if the model is not complex\n",
    "enough, it will underfit and may have large bias, again resulting in poor\n",
    "generalization. \n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
